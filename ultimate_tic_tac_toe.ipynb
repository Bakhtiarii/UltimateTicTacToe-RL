{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pksdmsyi/UltimateTicTacToe-RL/blob/main/ultimate_tic_tac_toe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# board.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Board:\n",
        "    def __init__(self):\n",
        "        # Initialize the overall 9x9 grid (9 sub-grids, each 3x3)\n",
        "        self.board = np.zeros((9, 9), dtype=int)  # Overall 9x9 board\n",
        "        self.subgrid_wins = [0] * 9  # Track wins for each subgrid: 0 = not won, 1 = Player 1, 2 = Player 2\n",
        "        self.overall_winner = 0  # Track the overall winner: 0 = none, 1 = Player 1, 2 = Player 2\n",
        "        self.next_subgrid = None  # Tracks the subgrid where the next move must be made\n",
        "        self.last_move = -1\n",
        "\n",
        "    def index_to_position(self, index, size):\n",
        "        \"\"\"\n",
        "        Convert a flat index into row and column coordinates.\n",
        "        For size = 3 (subgrid), index will be from 0-8.\n",
        "        For size = 9 (full board), index will be from 0-80.\n",
        "        \"\"\"\n",
        "        row = index // size\n",
        "        col = index % size\n",
        "        return row, col\n",
        "\n",
        "    def get_subgrid(self, subgrid_index):\n",
        "        \"\"\"\n",
        "        Get a specific 3x3 subgrid by index (0-8).\n",
        "        \"\"\"\n",
        "        row_start = (subgrid_index // 3) * 3  # Row of the top-left corner of the subgrid\n",
        "        col_start = (subgrid_index % 3) * 3   # Column of the top-left corner of the subgrid\n",
        "        return self.board[row_start:row_start + 3, col_start:col_start + 3]\n",
        "\n",
        "    def set_subgrid(self, subgrid_index, subgrid):\n",
        "        \"\"\"\n",
        "        Set the values of a specific 3x3 subgrid.\n",
        "        The subgrid should be a 3x3 numpy array.\n",
        "        \"\"\"\n",
        "        if subgrid.shape != (3, 3):\n",
        "            raise ValueError(\"Subgrid must be a 3x3 numpy array\")\n",
        "\n",
        "        row_start = (subgrid_index // 3) * 3\n",
        "        col_start = (subgrid_index % 3) * 3\n",
        "        self.board[row_start:row_start + 3, col_start:col_start + 3] = subgrid\n",
        "\n",
        "\n",
        "    def is_valid_move(self, row, col):\n",
        "        \"\"\"\n",
        "        Check if a move is valid by checking if the corresponding subgrid is available\n",
        "        and the cell is empty.\n",
        "        \"\"\"\n",
        "        # Check if move is in the valid subgrid, unless it's a free move\n",
        "        if self.next_subgrid is not None:\n",
        "            subgrid_row, subgrid_col = row // 3, col // 3\n",
        "            if subgrid_row * 3 + subgrid_col != self.next_subgrid:\n",
        "                return False\n",
        "\n",
        "        return self.board[row, col] == 0\n",
        "\n",
        "    def update_cell(self, index, value):\n",
        "        \"\"\"\n",
        "        Update a single cell in the 9x9 board using a flat index (0-80).\n",
        "        This method ensures that updates to the overall board are reflected in the corresponding subgrid.\n",
        "        \"\"\"\n",
        "        if not (0 <= index < 81):\n",
        "            raise ValueError(\"Index must be between 0 and 80.\")\n",
        "        if value not in (0, 1, 2):  # Assuming 0 = empty, 1 = Player 1, 2 = Player 2\n",
        "            raise ValueError(\"Cell value must be 0 (empty), 1 (Player 1), or 2 (Player 2).\")\n",
        "\n",
        "        # Convert the flat index (0-80) to a row and column\n",
        "        row, col = self.index_to_position(index, 9)\n",
        "\n",
        "        if not self.is_valid_move(row, col):\n",
        "            raise ValueError(f\"Invalid move: Cell at index {index} is already occupied.\")\n",
        "\n",
        "        # Update the main 9x9 board\n",
        "        self.board[row, col] = value\n",
        "\n",
        "        # Check the winner for the subgrid\n",
        "        subgrid_index = (row // 3) * 3 + (col // 3)\n",
        "        self.check_subgrid_winner(subgrid_index)\n",
        "\n",
        "        # Update the next subgrid based on this move\n",
        "        self.update_next_subgrid(row, col)\n",
        "\n",
        "        # Check overall winner after every move\n",
        "        self.check_winner(value)\n",
        "\n",
        "    def update_cell_in_subgrid(self, subgrid_index, subgrid_index_flat, value):\n",
        "        \"\"\"\n",
        "        Update a specific cell within a subgrid (3x3) by subgrid index (0-8) and\n",
        "        flat cell index (0-8), and reflect this change in the overall 9x9 board.\n",
        "\n",
        "        subgrid_index: The index of the subgrid (0-8).\n",
        "        subgrid_index_flat: The flat index within the subgrid (0-8).\n",
        "        value: The value to set (0 = empty, 1 = Player 1, 2 = Player 2).\n",
        "        \"\"\"\n",
        "        if not (0 <= subgrid_index < 9):\n",
        "            raise ValueError(\"Subgrid index must be between 0 and 8.\")\n",
        "        if not (0 <= subgrid_index_flat < 9):\n",
        "            raise ValueError(\"Subgrid cell index must be between 0 and 8.\")\n",
        "        if value not in (0, 1, 2):\n",
        "            raise ValueError(\"Cell value must be 0 (empty), 1 (Player 1), or 2 (Player 2).\")\n",
        "\n",
        "        # Convert the flat index within subgrid (0-8) to local row and column\n",
        "        local_row, local_col = self.index_to_position(subgrid_index_flat, 3)\n",
        "\n",
        "        # Calculate the global row and column in the 9x9 board based on the subgrid index\n",
        "        global_row = (subgrid_index // 3) * 3 + local_row\n",
        "        global_col = (subgrid_index % 3) * 3 + local_col\n",
        "\n",
        "        if not self.is_valid_move(global_row, global_col):\n",
        "            raise ValueError(f\"Invalid move: Cell in subgrid {subgrid_index}, index {subgrid_index_flat} is already occupied.\")\n",
        "\n",
        "        # Update the corresponding cell in the 9x9 board\n",
        "        self.board[global_row, global_col] = value\n",
        "\n",
        "        # Check the winner for the subgrid\n",
        "        self.check_subgrid_winner(subgrid_index)\n",
        "\n",
        "         # Update the next subgrid restriction based on the current move\n",
        "        self.update_next_subgrid(global_row, global_col)\n",
        "\n",
        "        # Check overall winner after every move\n",
        "        self.check_winner(value)\n",
        "\n",
        "    def check_winner(self, player):\n",
        "        \"\"\"\n",
        "        Check if the given player has won the overall board or any subgrid.\n",
        "        \"\"\"\n",
        "        # Check rows and columns for overall board\n",
        "        for i in range(9):\n",
        "            if all(self.board[i, j] == player for j in range(9)):  # Check row\n",
        "                self.overall_winner = player\n",
        "                return True\n",
        "            if all(self.board[j, i] == player for j in range(9)):  # Check column\n",
        "                self.overall_winner = player\n",
        "                return True\n",
        "\n",
        "        # Check diagonals for overall board\n",
        "        if all(self.board[i, i] == player for i in range(9)):  # Main diagonal\n",
        "            self.overall_winner = player\n",
        "            return True\n",
        "        if all(self.board[i, 8 - i] == player for i in range(9)):  # Anti-diagonal\n",
        "            self.overall_winner = player\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def check_subgrid_winner(self, subgrid_index):\n",
        "        \"\"\"\n",
        "        Check if there is a winner in a specific subgrid.\n",
        "        \"\"\"\n",
        "        subgrid = self.get_subgrid(subgrid_index)\n",
        "\n",
        "        for player in [1, 2]:\n",
        "            # Check rows and columns for subgrid\n",
        "            for i in range(3):\n",
        "                if all(subgrid[i, j] == player for j in range(3)):  # Check row\n",
        "                    self.subgrid_wins[subgrid_index] = player\n",
        "                    return player\n",
        "                if all(subgrid[j, i] == player for j in range(3)):  # Check column\n",
        "                    self.subgrid_wins[subgrid_index] = player\n",
        "                    return player\n",
        "\n",
        "            # Check diagonals for subgrid\n",
        "            if all(subgrid[i, i] == player for i in range(3)):  # Main diagonal\n",
        "                self.subgrid_wins[subgrid_index] = player\n",
        "                return player\n",
        "            if all(subgrid[i, 2 - i] == player for i in range(3)):  # Anti-diagonal\n",
        "                self.subgrid_wins[subgrid_index] = player\n",
        "                return player\n",
        "\n",
        "        return None  # No winner in this subgrid\n",
        "\n",
        "    def is_subgrid_full(self, subgrid_index):\n",
        "        \"\"\"\n",
        "        Check if a specific subgrid is full (i.e., no available moves).\n",
        "        \"\"\"\n",
        "        subgrid = self.get_subgrid(subgrid_index)\n",
        "        return np.all(subgrid != 0)\n",
        "\n",
        "    def update_next_subgrid(self, row, col):\n",
        "        \"\"\"\n",
        "        Update the next subgrid based on the position of the last move.\n",
        "        \"\"\"\n",
        "        # Determine the subgrid index where the next move should be made\n",
        "        next_subgrid = (row % 3) * 3 + (col % 3)\n",
        "        if self.is_subgrid_full(next_subgrid) or self.subgrid_wins[next_subgrid] != 0:\n",
        "            self.next_subgrid = None  # Free play if the next subgrid is full or won\n",
        "        else:\n",
        "            self.next_subgrid = next_subgrid\n",
        "\n",
        "    def display_winners(self):\n",
        "        \"\"\"\n",
        "        Display the winners for subgrids and the overall board.\n",
        "        \"\"\"\n",
        "        print(\"Subgrid Winners:\")\n",
        "        for i, winner in enumerate(self.subgrid_wins):\n",
        "            if winner == 0:\n",
        "                print(f\"Subgrid {i}: No winner\")\n",
        "            else:\n",
        "                print(f\"Subgrid {i}: Player {winner} wins\")\n",
        "\n",
        "        if self.overall_winner:\n",
        "            print(f\"\\nOverall Winner: Player {self.overall_winner}\")\n",
        "        else:\n",
        "            print(\"\\nOverall Winner: None\")\n",
        "\n",
        "    def print_board(self):\n",
        "        \"\"\"\n",
        "        Print the 9x9 overall board with separation between subgrids.\n",
        "        \"\"\"\n",
        "        print(\"Ultimate Tic-Tac-Toe Board (9x9):\\n\")\n",
        "        for i in range(3):  # Iterate over 3 rows of subgrids\n",
        "            for row in range(3):  # Each row within subgrid\n",
        "                row_display = \"\"\n",
        "                for j in range(3):  # Iterate over 3 columns of subgrids\n",
        "                    row_display += \" \".join(map(str, self.board[i * 3 + row, j * 3:j * 3 + 3])) + \" | \"\n",
        "                print(row_display)\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "    def plot_board(self):\n",
        "        \"\"\"\n",
        "        Plot the current state of the 9x9 board using matplotlib.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.title(\"Ultimate Tic-Tac-Toe Board\")\n",
        "\n",
        "        # Create the grid\n",
        "        for i in range(10):\n",
        "            # Thicker lines for 3x3 subgrid borders\n",
        "            linewidth = 2 if i % 3 == 0 else 0.5\n",
        "            plt.axhline(i, color='black', linewidth=linewidth, linestyle='-')\n",
        "            plt.axvline(i, color='black', linewidth=linewidth, linestyle='-')\n",
        "\n",
        "        # Set the ticks and labels\n",
        "        plt.xticks(np.arange(0.5, 9, 1), [])\n",
        "        plt.yticks(np.arange(0.5, 9, 1), [])\n",
        "\n",
        "        # Fill in the board with markers\n",
        "        for i in range(9):\n",
        "            for j in range(9):\n",
        "                # Center the markers in their respective cells\n",
        "                center_x = j + 0.5\n",
        "                center_y = 8.45 - i\n",
        "                if self.board[i, j] == 1:\n",
        "                    plt.text(center_x, center_y, 'X', fontsize=40, ha='center', va='center', color='blue')\n",
        "                elif self.board[i, j] == 2:\n",
        "                    plt.text(center_x, center_y, 'O', fontsize=40, ha='center', va='center', color='red')\n",
        "\n",
        "        plt.xlim(0, 9)\n",
        "        plt.ylim(0, 9)\n",
        "        plt.grid(False)\n",
        "        plt.gca().set_aspect('equal', adjustable='box')  # Maintain aspect ratio\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "7tCYaq_OT50D"
      },
      "id": "7tCYaq_OT50D",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class UltimateTicTacToeEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(UltimateTicTacToeEnv, self).__init__()\n",
        "        self.board = Board()\n",
        "        self.action_space = spaces.Discrete(81)  # 9x9 board with 81 cells\n",
        "        self.observation_space = spaces.Box(low=0, high=2, shape=(9, 9), dtype=int)\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = Board()\n",
        "        return self.board.board.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, 9)\n",
        "        player = 1  # Assuming the RL agent is Player 1\n",
        "\n",
        "        if not self.board.is_valid_move(row, col):\n",
        "            return self.board.board.flatten(), -1, True, {}  # Invalid move penalty\n",
        "\n",
        "        # Update the Last action variable\n",
        "        self.board.last_move = action\n",
        "\n",
        "        # Update the cell and check if we have a winner\n",
        "        self.board.update_cell(action, player)\n",
        "\n",
        "        done = self.board.overall_winner != 0 or np.all(self.board.board != 0)  # Win or full board\n",
        "\n",
        "        reward = 1 if self.board.overall_winner == player else 0  # Reward if agent wins\n",
        "        return self.board.board.flatten(), reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        self.board.print_board()\n"
      ],
      "metadata": {
        "id": "o68pHtloTvJK"
      },
      "id": "o68pHtloTvJK",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # Discount factor\n",
        "        self.epsilon = 1.0  # Exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.action_size)\n",
        "        )\n",
        "\n",
        "    def remember(self, last_move, state, action, reward, next_state, done):\n",
        "        self.memory.append((last_move, state, action, reward, next_state, done))\n",
        "\n",
        "    def get_next_subgrid(self, previous_move):\n",
        "        # Determine the row and column within the 9x9 board (0-indexed)\n",
        "        board_row, board_col = divmod(previous_move, 9)\n",
        "\n",
        "        # Calculate the row and column within the current 3x3 subgrid\n",
        "        subgrid_row = board_row % 3\n",
        "        subgrid_col = board_col % 3\n",
        "\n",
        "        # Calculate the next subgrid index (0-8)\n",
        "        next_subgrid_index = subgrid_row * 3 + subgrid_col\n",
        "        return next_subgrid_index\n",
        "\n",
        "    def get_valid_actions(self, board, last_move):\n",
        "        # This function should return a list of valid actions based on the `last_move`.\n",
        "        # For example, in Ultimate Tic Tac Toe, it might return the set of moves allowed in the specific sub-board indicated by last_move.\n",
        "        # You will need to implement this according to the game's rules.\n",
        "        # Here is a placeholder implementation:\n",
        "        valid_actions = []\n",
        "        target_sub_board = self.get_next_subgrid(last_move)  # Assuming sub-boards are indexed 0 to 8\n",
        "\n",
        "        # Calculate the start index of the target sub-board\n",
        "        start_row = (target_sub_board // 3) * 3\n",
        "        start_col = (target_sub_board % 3) * 3\n",
        "\n",
        "        # Loop through the cells in the target sub-board\n",
        "        for row in range(start_row, start_row + 3):\n",
        "            for col in range(start_col, start_col + 3):\n",
        "                action = row * 9 + col  # Calculate the flat index (0-80)\n",
        "                if board[0][action] == 0:  # Check if the cell is empty\n",
        "                    valid_actions.append(action)\n",
        "\n",
        "        # Return Valid Actions if There are any valid actions possible for that Subgrid\n",
        "        if valid_actions:\n",
        "          return valid_actions\n",
        "\n",
        "        # Else Return all the valid actions in the board\n",
        "        valid_actions = []\n",
        "        for row in range(9):\n",
        "            for col in range(9):\n",
        "                action = row * 9 + col  # Calculate the flat index (0-80)\n",
        "                if board[0][action] == 0:  # Check if the cell is empty\n",
        "                    valid_actions.append(action)\n",
        "\n",
        "        return valid_actions\n",
        "\n",
        "    def act(self, state, last_move):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # Choose a random valid action based on last_move\n",
        "            return random.choice(self.get_valid_actions(state, last_move))\n",
        "\n",
        "        # Combine state and last_move as inputs\n",
        "        combined_input = np.concatenate((state, [[last_move]]), axis=1)\n",
        "        state_tensor = torch.FloatTensor(combined_input).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state_tensor)\n",
        "\n",
        "        # Masking invalid actions based on last_move\n",
        "        valid_actions = self.get_valid_actions(state, last_move)\n",
        "        q_values = q_values.cpu().data.numpy()[0][0]\n",
        "        # print(q_values.shape,last_move//9, valid_actions)\n",
        "        masked_q_values = [q if i in valid_actions else -np.inf for i, q in enumerate(q_values)]\n",
        "        # print(\"Predicted Action \", np.argmax(masked_q_values), masked_q_values)\n",
        "        return np.argmax(masked_q_values)\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "      minibatch = random.sample(self.memory, batch_size)\n",
        "      criterion = nn.MSELoss()\n",
        "      optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "      for last_move, state, action, reward, next_state, done in minibatch:\n",
        "          target = reward\n",
        "          if not done:\n",
        "              next_state_tensor = torch.FloatTensor(np.concatenate((next_state, [[action]]), axis=1)).unsqueeze(0)\n",
        "              target = reward + self.gamma * torch.max(self.model(next_state_tensor)).item()\n",
        "\n",
        "          state_tensor = torch.FloatTensor(np.concatenate((state, [[last_move]]), axis=1)).unsqueeze(0)\n",
        "          q_values = self.model(state_tensor)\n",
        "          current_q_value = q_values[0][0][action]\n",
        "\n",
        "          # Convert target to a FloatTensor\n",
        "          target_tensor = torch.FloatTensor([target])\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss = criterion(current_q_value, target_tensor)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "          self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "pHtMMoiKTvf9"
      },
      "id": "pHtMMoiKTvf9",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = UltimateTicTacToeEnv()\n",
        "agent = DQNAgent(state_size=82, action_size=81)\n",
        "episodes = 100\n",
        "batch_size = 32\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, 81])\n",
        "\n",
        "    for time in range(500):\n",
        "        last_move = env.board.last_move\n",
        "        action = agent.act(state, last_move)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, 81])\n",
        "        agent.remember(last_move, state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes}, Score: {time}, Epsilon: {agent.epsilon}\")\n",
        "            break\n",
        "\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "6zS6ongvTydh"
      },
      "id": "6zS6ongvTydh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGENT VS ME"
      ],
      "metadata": {
        "id": "BofLefuwfhP_"
      },
      "id": "BofLefuwfhP_"
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, set epsilon to 0 so the agent uses its learned policy\n",
        "agent.epsilon = 0\n",
        "\n",
        "# Initialize the environment\n",
        "env = UltimateTicTacToeEnv()\n",
        "state = env.reset()\n",
        "state = np.reshape(state, [1, 81])\n",
        "last_move = -1\n",
        "\n",
        "done = False\n",
        "\n",
        "print(\"\\nStarting a new game of Ultimate Tic-Tac-Toe!\")\n",
        "print(\"You are Player 2 (O), and the agent is Player 1 (X).\")\n",
        "print(\"Enter your moves by typing a number between 0 and 80 corresponding to the cell on the 9x9 board.\\n\")\n",
        "\n",
        "while not done:\n",
        "    # Agent's turn\n",
        "    print(\"Agent's Turn:\")\n",
        "\n",
        "    valid_move = False\n",
        "    # while not valid_move:\n",
        "    action = agent.act(state, last_move)\n",
        "\n",
        "      # row, col = divmod(action, 9)\n",
        "      # if env.board.is_valid_move(row, col):\n",
        "      #     valid_move = True\n",
        "      # else:\n",
        "      #   pass\n",
        "\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    state = np.reshape(next_state, [1, 81])\n",
        "    env.render()\n",
        "    if done:\n",
        "        if reward == 1:\n",
        "            print(\"Agent wins!\")\n",
        "        else:\n",
        "            print(\"It's a tie!\")\n",
        "        break\n",
        "\n",
        "    # User's turn\n",
        "    valid_move = False\n",
        "    while not valid_move:\n",
        "        user_input = input(\"Your Turn. Enter your move (0-80): \")\n",
        "        try:\n",
        "            user_action = int(user_input)\n",
        "            if not (0 <= user_action < 81):\n",
        "                print(\"Invalid input. Enter a number between 0 and 80.\")\n",
        "                continue\n",
        "            row, col = divmod(user_action, 9)\n",
        "            if env.board.is_valid_move(row, col):\n",
        "                valid_move = True\n",
        "            else:\n",
        "                print(\"Invalid move. The cell is either occupied or not in the valid subgrid. Try again.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Enter a number between 0 and 80.\")\n",
        "\n",
        "    last_move = user_action\n",
        "    # Update the board with the user's move\n",
        "    env.board.update_cell(user_action, 2)  # User is Player 2\n",
        "    env.board.print_board()\n",
        "\n",
        "    # Check if game is over after user's move\n",
        "    if env.board.overall_winner != 0 or np.all(env.board.board != 0):\n",
        "        done = True\n",
        "        if env.board.overall_winner == 2:\n",
        "            print(\"Congratulations! You win!\")\n",
        "        elif env.board.overall_winner == 1:\n",
        "            print(\"Agent wins!\")\n",
        "        else:\n",
        "            print(\"It's a tie!\")\n",
        "        break\n",
        "\n",
        "    # Update the state for the agent\n",
        "    state = env.board.board.flatten()\n",
        "    state = np.reshape(state, [1, 81])\n",
        "\n",
        "print(\"\\nGame Over.\")\n"
      ],
      "metadata": {
        "id": "_0r6SUF3T1Wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9241f2-099c-4854-b49d-b966b1b14402"
      },
      "id": "_0r6SUF3T1Wv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting a new game of Ultimate Tic-Tac-Toe!\n",
            "You are Player 2 (O), and the agent is Player 1 (X).\n",
            "Enter your moves by typing a number between 0 and 80 corresponding to the cell on the 9x9 board.\n",
            "\n",
            "Agent's Turn:\n",
            "(81,) -1 [60, 61, 62, 69, 70, 71, 78, 79, 80]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 1 0 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 67\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 1 0 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 7 [30, 31, 32, 39, 40, 41, 48, 49, 50]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 1 0 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 80\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 8 [60, 61, 62, 69, 70, 71, 78]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 72\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 8 [54, 55, 56, 63, 64, 65, 73, 74]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 0 | \n",
            "2 0 1 | 0 0 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 71\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 2 | \n",
            "2 0 1 | 0 0 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 7 [33, 34, 35, 42, 43, 44, 51, 52, 53]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 2 | \n",
            "2 0 1 | 0 0 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 76\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 2 | \n",
            "2 0 1 | 0 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 8 [57, 58, 59, 66, 68, 75, 77]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 2 | \n",
            "2 0 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 73\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 2 0 | 0 0 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 8 [57, 58, 59, 66, 68, 77]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 1 2 0 | 0 0 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 36\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 1 2 0 | 0 0 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 4 [27, 28, 29, 37, 38, 45, 46, 47]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 1 2 0 | 0 0 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 60\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 0 0 | \n",
            "0 0 0 | 1 2 0 | 0 0 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 6 [0, 1, 2, 9, 10, 11, 18, 19, 20]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 0 0 | \n",
            "0 0 0 | 1 2 0 | 0 0 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 70\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 0 0 | \n",
            "0 0 0 | 1 2 0 | 0 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 7 [30, 31, 32, 39, 40, 41, 48, 49]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 1 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 0 0 | \n",
            "0 0 0 | 1 2 0 | 0 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 65\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 1 0 1 | 0 1 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 0 0 | \n",
            "0 0 2 | 1 2 0 | 0 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 7 [33, 34, 35, 42, 43, 44, 51, 53]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 0 0 | \n",
            "0 0 2 | 1 2 0 | 0 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 62\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 0 | 0 0 0 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 0 2 | \n",
            "0 0 2 | 1 2 0 | 0 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 6 [6, 7, 8, 15, 16, 17, 24, 25, 26]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 0 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 0 2 | \n",
            "0 0 2 | 1 2 0 | 0 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 61\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 0 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 2 2 | \n",
            "0 0 2 | 1 2 0 | 0 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 6 [3, 4, 5, 12, 13, 14, 21, 22, 23]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 2 2 | \n",
            "0 0 2 | 1 2 0 | 0 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 69\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 2 2 | \n",
            "0 0 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 7 [27, 28, 29, 37, 38, 45, 46]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 2 2 2 | \n",
            "0 0 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 54\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 1 | 0 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 0 0 | 2 2 2 | \n",
            "0 0 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 6 [0, 1, 2, 9, 10, 11, 18, 19]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 1 1 | 0 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 0 0 | 2 2 2 | \n",
            "0 0 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 58\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 1 1 | 0 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "0 0 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 6 [3, 4, 5, 12, 13, 14, 21, 22]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "0 0 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 64\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "0 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 7 [30, 31, 32, 39, 40, 41, 49]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "0 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 0\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "0 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 0 [1, 2, 9, 10, 11, 18]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "0 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 63\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 7 [27, 28, 29, 37, 38, 46]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 0 0 | 0 0 0 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 44\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 0 0 | 0 0 2 | \n",
            "1 0 1 | 1 0 1 | 0 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 4 [33, 34, 35, 42, 43, 51]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 0 0 | 0 0 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 0 0 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 56\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 0 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 0 0 | 0 0 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 0 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 6 [6, 7, 8, 15, 16, 17, 24, 25]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 0 0 | 0 0 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 0 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 55\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 0 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 0 0 | 0 0 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 6 [3, 4, 5, 12, 13, 14, 22]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 0 0 | 0 0 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 40\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 2 0 | 0 0 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 4 [31, 32, 39, 41, 49]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 2 1 | 0 0 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 43\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 0 | 0 0 0 | \n",
            "2 0 1 | 0 2 1 | 0 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 4 [31, 32, 39, 49]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 0 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 1 | 0 0 0 | \n",
            "2 0 1 | 0 2 1 | 0 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 8\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 0 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 1 | 0 0 0 | \n",
            "2 0 1 | 0 2 1 | 0 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 0 [6, 7, 15, 16, 17, 25]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 1 | 0 0 0 | \n",
            "2 0 1 | 0 2 1 | 0 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 68\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 1 | 0 0 0 | \n",
            "2 0 1 | 0 2 1 | 0 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 7 [33, 34, 35, 42]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 0 0 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 0 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 17\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 0 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 0 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 1 [33, 34, 42]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 0 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 0 0 | 1 0 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 28\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 0 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 0 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 0 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 3 [3, 4, 5, 12, 14, 22]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 1 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 0 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 0 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 16\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 1 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 0 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 1 [31, 39, 49]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 0 1 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 1 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 4\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 0 1 0 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 1 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 0 [3, 12, 14, 22]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 0 1 1 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 1 1 | 0 0 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 34\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 0 1 1 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 1 1 | 0 2 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 3 [3, 12, 22]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 1 1 1 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 1 1 | 0 2 1 | \n",
            "2 0 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 37\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 1 1 1 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 1 1 | 0 2 1 | \n",
            "2 2 1 | 0 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 4 [39, 49]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 1 1 1 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 0 | 1 1 1 | 0 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 29\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 1 1 1 | 0 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 2 | 1 1 1 | 0 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 3 [6, 7, 15]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "0 2 2 | 1 1 1 | 0 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 27\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 0 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 1 1 | 0 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 3 [1, 2, 9, 10, 11]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 1 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 1 1 | 0 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 33\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "0 0 1 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 1 1 | 2 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 3 [1, 2, 9, 10]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "1 0 1 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 1 1 | 2 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 0 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 46\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "1 0 1 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 1 1 | 2 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 2 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 0 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 5 [57, 59, 77]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 0 0 | 0 2 1 | 0 0 2 | \n",
            "1 0 1 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 1 1 | 2 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 2 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Your Turn. Enter your move (0-80): 1\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 2 0 | 0 2 1 | 0 0 2 | \n",
            "1 0 1 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 1 1 | 2 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 2 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent's Turn:\n",
            "(81,) 0 [3, 22]\n",
            "Ultimate Tic-Tac-Toe Board (9x9):\n",
            "\n",
            "2 2 0 | 1 2 1 | 0 0 2 | \n",
            "1 0 1 | 1 1 1 | 1 2 2 | \n",
            "1 1 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 1 1 | 2 2 1 | \n",
            "2 2 1 | 1 2 1 | 1 2 2 | \n",
            "1 2 1 | 1 0 1 | 1 1 1 | \n",
            "--------------------\n",
            "2 2 2 | 1 2 0 | 2 2 2 | \n",
            "2 2 2 | 1 2 2 | 2 2 2 | \n",
            "2 2 1 | 1 2 0 | 1 1 2 | \n",
            "--------------------\n",
            "Agent wins!\n",
            "\n",
            "Game Over.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGENT VS RANDOM BEST SEED"
      ],
      "metadata": {
        "id": "qYah-9W2fcoS"
      },
      "id": "qYah-9W2fcoS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent 1 With Complete Model Exploitation"
      ],
      "metadata": {
        "id": "I0qyFBGFhQ7H"
      },
      "id": "I0qyFBGFhQ7H"
    },
    {
      "cell_type": "code",
      "source": [
        "env_agent = UltimateTicTacToeEnv()\n",
        "model_path = \"dqn_agent_weights.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n",
        "agent = DQNAgent(state_size=82, action_size=81)\n",
        "agent.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "jne3YyTFkVvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9e6e36-7fdc-4da4-e653-45699a1fd1a1"
      },
      "id": "jne3YyTFkVvN",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-a074415e080c>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent 2 With Complete Model Exploration"
      ],
      "metadata": {
        "id": "mj3pwLhohUYf"
      },
      "id": "mj3pwLhohUYf"
    },
    {
      "cell_type": "code",
      "source": [
        "env_random = UltimateTicTacToeEnv()\n",
        "model_path = \"dqn_agent_weights.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n",
        "random_agent = DQNAgent(state_size=82, action_size=81)\n",
        "random_agent.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqCwmxpMhTyJ",
        "outputId": "dd6f4a7e-6dea-4980-ddf4-0a8fa6cc2c95"
      },
      "id": "hqCwmxpMhTyJ",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-85fefecb8ce1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, set epsilon to 0 so the agent uses its learned policy\n",
        "agent.epsilon = 0\n",
        "# After training, set epsilon to 1 so the random agent never uses its learned policy\n",
        "random_agent.epsilon = 1\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#Initialize a counter\n",
        "ctr_agent_max = 0\n",
        "ctr_draw_max = 0\n",
        "ctr_random_max = 0\n",
        "best_seed = 0\n",
        "\n",
        "for seed in range(101):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "  #Initialize a counter\n",
        "  ctr_agent = 0\n",
        "  ctr_draw = 0\n",
        "  ctr_random = 0\n",
        "\n",
        "  for _ in range(100) :\n",
        "    # Initialize the environment\n",
        "    state_agent = env_agent.reset()\n",
        "    state_agent = np.reshape(state_agent, [1, 81])\n",
        "    state_random = env_random.reset()\n",
        "    state_random = np.reshape(state_random, [1, 81])\n",
        "    last_move = -1\n",
        "\n",
        "    done = False\n",
        "\n",
        "    # print(\"\\nStarting a new game of Ultimate Tic-Tac-Toe!\")\n",
        "    # print(\"Random is Player 2 (O), and the Agent is Player 1 (X).\")\n",
        "\n",
        "    while not done:\n",
        "        # Agent's turn\n",
        "        # print(\"\\nAgent's Turn:\")\n",
        "        agent_action = agent.act(state_agent, last_move)\n",
        "\n",
        "        # Apply Agent's action\n",
        "        next_state, reward, done, _ = env_agent.step(agent_action)\n",
        "        state_agent = np.reshape(next_state, [1, 81])\n",
        "\n",
        "        # Update the last move and synchronize Random's board\n",
        "        last_move = agent_action\n",
        "        env_random.board.update_cell(agent_action, 2)  # Agent is Player 1 in Random's board\n",
        "\n",
        "        # Update Random Agent's state\n",
        "        state_random = env_random.board.board.flatten()\n",
        "        state_random = np.reshape(state_random, [1, 81])\n",
        "\n",
        "        # Print the boards\n",
        "        # print(\"\\nAgent's Board:\")\n",
        "        # env_agent.render()\n",
        "        # print(\"\\nRandom Agent's Board:\")\n",
        "        # env_random.render()\n",
        "\n",
        "        if done:\n",
        "            if reward == 1:\n",
        "                ctr_agent+=1\n",
        "                # print(\"Agent wins!\")\n",
        "            else:\n",
        "                ctr_draw+=1\n",
        "                # print(\"It's a tie!\")\n",
        "            break\n",
        "\n",
        "        # Random Agent's turn\n",
        "        # print(\"\\nRandom Agent's Turn:\")\n",
        "        random_action = random_agent.act(state_random, last_move)\n",
        "\n",
        "        # Apply Random Agent's action\n",
        "        next_state, reward, done, _ = env_random.step(random_action)\n",
        "        state_random = np.reshape(next_state, [1, 81])\n",
        "\n",
        "        # Synchronize Agent's board\n",
        "        env_agent.board.update_cell(random_action, 2)  # Random is Player 2 in Agent's board\n",
        "\n",
        "        # Print the boards\n",
        "        # print(\"\\nAgent's Board:\")\n",
        "        # env_agent.render()\n",
        "        # print(\"\\nRandom Agent's Board:\")\n",
        "        # env_random.render()\n",
        "\n",
        "        if done:\n",
        "            if reward == 1:\n",
        "                ctr_random+=1\n",
        "                # print(\"Random Agent wins!\")\n",
        "            else:\n",
        "                ctr_draw+=1\n",
        "                # print(\"It's a tie!\")\n",
        "            break\n",
        "\n",
        "        # Update the last move and Agent's state\n",
        "        last_move = random_action\n",
        "        state_agent = env_agent.board.board.flatten()\n",
        "        state_agent = np.reshape(state_agent, [1, 81])\n",
        "\n",
        "    # print(\"\\nGame Over.\")\n",
        "\n",
        "    if ctr_agent > ctr_agent_max:\n",
        "      ctr_agent_max = ctr_agent\n",
        "      ctr_draw_max = ctr_draw\n",
        "      ctr_random_max = ctr_random\n",
        "      best_seed = seed\n"
      ],
      "metadata": {
        "id": "eKNsVfbZf2B9"
      },
      "execution_count": 75,
      "outputs": [],
      "id": "eKNsVfbZf2B9"
    },
    {
      "cell_type": "code",
      "source": [
        "print(ctr_agent_max, ctr_draw_max, ctr_random_max, best_seed)"
      ],
      "metadata": {
        "id": "XBuJmE44jQuv",
        "outputId": "5828827e-efa6-4783-e8a8-bbcb6437075d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XBuJmE44jQuv",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 68 6 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGENT VS RANDOM"
      ],
      "metadata": {
        "id": "sCC0xkMw4oMy"
      },
      "id": "sCC0xkMw4oMy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent 1 With Complete Model Exploitation"
      ],
      "metadata": {
        "id": "Mn1zcqBq4oMy"
      },
      "id": "Mn1zcqBq4oMy"
    },
    {
      "cell_type": "code",
      "source": [
        "env_agent = UltimateTicTacToeEnv()\n",
        "model_path = \"dqn_agent_weights.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n",
        "agent = DQNAgent(state_size=82, action_size=81)\n",
        "agent.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946f300b-96eb-47e7-89c3-63c232be04f8",
        "id": "K0NxtbS04oMz"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-78-a074415e080c>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "id": "K0NxtbS04oMz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent 2 With Complete Model Exploration"
      ],
      "metadata": {
        "id": "OSKGDO_X4oMz"
      },
      "id": "OSKGDO_X4oMz"
    },
    {
      "cell_type": "code",
      "source": [
        "env_random = UltimateTicTacToeEnv()\n",
        "model_path = \"dqn_agent_weights.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n",
        "random_agent = DQNAgent(state_size=82, action_size=81)\n",
        "random_agent.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "outputId": "c87063be-acd2-4645-dc72-8f7312266e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnur4-YH4oMz"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-85fefecb8ce1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "id": "Pnur4-YH4oMz"
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, set epsilon to 0 so the agent uses its learned policy\n",
        "agent.epsilon = 0\n",
        "# After training, set epsilon to 1 so the random agent never uses its learned policy\n",
        "random_agent.epsilon = 1\n",
        "\n",
        "random.seed(16)\n",
        "np.random.seed(16)\n",
        "torch.manual_seed(16)\n",
        "\n",
        "#Initialize a counter\n",
        "ctr_agent = 0\n",
        "ctr_draw = 0\n",
        "ctr_random = 0\n",
        "\n",
        "\n",
        "for _ in range(100) :\n",
        "  # Initialize the environment\n",
        "  state_agent = env_agent.reset()\n",
        "  state_agent = np.reshape(state_agent, [1, 81])\n",
        "  state_random = env_random.reset()\n",
        "  state_random = np.reshape(state_random, [1, 81])\n",
        "  last_move = -1\n",
        "\n",
        "  done = False\n",
        "\n",
        "  # print(\"\\nStarting a new game of Ultimate Tic-Tac-Toe!\")\n",
        "  # print(\"Random is Player 2 (O), and the Agent is Player 1 (X).\")\n",
        "\n",
        "  while not done:\n",
        "      # Agent's turn\n",
        "      # print(\"\\nAgent's Turn:\")\n",
        "      agent_action = agent.act(state_agent, last_move)\n",
        "\n",
        "      # Apply Agent's action\n",
        "      next_state, reward, done, _ = env_agent.step(agent_action)\n",
        "      state_agent = np.reshape(next_state, [1, 81])\n",
        "\n",
        "      # Update the last move and synchronize Random's board\n",
        "      last_move = agent_action\n",
        "      env_random.board.update_cell(agent_action, 2)  # Agent is Player 1 in Random's board\n",
        "\n",
        "      # Update Random Agent's state\n",
        "      state_random = env_random.board.board.flatten()\n",
        "      state_random = np.reshape(state_random, [1, 81])\n",
        "\n",
        "      # Print the boards\n",
        "      # print(\"\\nAgent's Board:\")\n",
        "      # env_agent.render()\n",
        "      # print(\"\\nRandom Agent's Board:\")\n",
        "      # env_random.render()\n",
        "\n",
        "      if done:\n",
        "          if reward == 1:\n",
        "              ctr_agent+=1\n",
        "              # print(\"Agent wins!\")\n",
        "          else:\n",
        "              ctr_draw+=1\n",
        "              # print(\"It's a tie!\")\n",
        "          break\n",
        "\n",
        "      # Random Agent's turn\n",
        "      # print(\"\\nRandom Agent's Turn:\")\n",
        "      random_action = random_agent.act(state_random, last_move)\n",
        "\n",
        "      # Apply Random Agent's action\n",
        "      next_state, reward, done, _ = env_random.step(random_action)\n",
        "      state_random = np.reshape(next_state, [1, 81])\n",
        "\n",
        "      # Synchronize Agent's board\n",
        "      env_agent.board.update_cell(random_action, 2)  # Random is Player 2 in Agent's board\n",
        "\n",
        "      # Print the boards\n",
        "      # print(\"\\nAgent's Board:\")\n",
        "      # env_agent.render()\n",
        "      # print(\"\\nRandom Agent's Board:\")\n",
        "      # env_random.render()\n",
        "\n",
        "      if done:\n",
        "          if reward == 1:\n",
        "              ctr_random+=1\n",
        "              # print(\"Random Agent wins!\")\n",
        "          else:\n",
        "              ctr_draw+=1\n",
        "              # print(\"It's a tie!\")\n",
        "          break\n",
        "\n",
        "      # Update the last move and Agent's state\n",
        "      last_move = random_action\n",
        "      state_agent = env_agent.board.board.flatten()\n",
        "      state_agent = np.reshape(state_agent, [1, 81])\n",
        "\n",
        "  # print(\"\\nGame Over.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "12atzNWx4oMz"
      },
      "execution_count": 80,
      "outputs": [],
      "id": "12atzNWx4oMz"
    },
    {
      "cell_type": "code",
      "source": [
        "print(ctr_agent, ctr_draw, ctr_random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC3h3uB807OO",
        "outputId": "614ebb5b-8976-46c7-8a9f-ac67f6efb37c"
      },
      "id": "BC3h3uB807OO",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 68 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGENT VS AGENT"
      ],
      "metadata": {
        "id": "kvseznPa5AMl"
      },
      "id": "kvseznPa5AMl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent 1 With Complete Model Exploitation"
      ],
      "metadata": {
        "id": "gssTZnnY5AMl"
      },
      "id": "gssTZnnY5AMl"
    },
    {
      "cell_type": "code",
      "source": [
        "env_agent = UltimateTicTacToeEnv()\n",
        "model_path = \"dqn_agent_weights.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n",
        "agent = DQNAgent(state_size=82, action_size=81)\n",
        "agent.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a267df3-1e1c-4d30-dfc4-bc61cc41599a",
        "id": "GL5G6kg-5AMl"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-a074415e080c>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "id": "GL5G6kg-5AMl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent 2 With Complete Model Exploration"
      ],
      "metadata": {
        "id": "Qo1JZvlX5AMm"
      },
      "id": "Qo1JZvlX5AMm"
    },
    {
      "cell_type": "code",
      "source": [
        "env_random = UltimateTicTacToeEnv()\n",
        "model_path = \"dqn_agent_weights.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n",
        "random_agent = DQNAgent(state_size=82, action_size=81)\n",
        "random_agent.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "outputId": "80f8fc4a-5f77-423d-987c-5fba61545307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Wtkgxg5AMm"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-85fefecb8ce1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "id": "d0Wtkgxg5AMm"
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, set epsilon to 0 so the agent uses its learned policy\n",
        "agent.epsilon = 0\n",
        "# After training, set epsilon to 1 so the random agent never uses its learned policy\n",
        "random_agent.epsilon = 0\n",
        "\n",
        "#Initialize a counter\n",
        "ctr_agent = 0\n",
        "ctr_draw = 0\n",
        "ctr_random = 0\n",
        "\n",
        "\n",
        "for _ in range(100) :\n",
        "  # Initialize the environment\n",
        "  state_agent = env_agent.reset()\n",
        "  state_agent = np.reshape(state_agent, [1, 81])\n",
        "  state_random = env_random.reset()\n",
        "  state_random = np.reshape(state_random, [1, 81])\n",
        "  last_move = -1\n",
        "\n",
        "  done = False\n",
        "\n",
        "  # print(\"\\nStarting a new game of Ultimate Tic-Tac-Toe!\")\n",
        "  # print(\"Random is Player 2 (O), and the Agent is Player 1 (X).\")\n",
        "\n",
        "  while not done:\n",
        "      # Agent's turn\n",
        "      # print(\"\\nAgent's Turn:\")\n",
        "      agent_action = agent.act(state_agent, last_move)\n",
        "\n",
        "      # Apply Agent's action\n",
        "      next_state, reward, done, _ = env_agent.step(agent_action)\n",
        "      state_agent = np.reshape(next_state, [1, 81])\n",
        "\n",
        "      # Update the last move and synchronize Random's board\n",
        "      last_move = agent_action\n",
        "      env_random.board.update_cell(agent_action, 2)  # Agent is Player 1 in Random's board\n",
        "\n",
        "      # Update Random Agent's state\n",
        "      state_random = env_random.board.board.flatten()\n",
        "      state_random = np.reshape(state_random, [1, 81])\n",
        "\n",
        "      # Print the boards\n",
        "      # print(\"\\nAgent's Board:\")\n",
        "      # env_agent.render()\n",
        "      # print(\"\\nRandom Agent's Board:\")\n",
        "      # env_random.render()\n",
        "\n",
        "      if done:\n",
        "          if reward == 1:\n",
        "              ctr_agent+=1\n",
        "              # print(\"Agent wins!\")\n",
        "          else:\n",
        "              ctr_draw+=1\n",
        "              # print(\"It's a tie!\")\n",
        "          break\n",
        "\n",
        "      # Random Agent's turn\n",
        "      # print(\"\\nRandom Agent's Turn:\")\n",
        "      random_action = random_agent.act(state_random, last_move)\n",
        "\n",
        "      # Apply Random Agent's action\n",
        "      next_state, reward, done, _ = env_random.step(random_action)\n",
        "      state_random = np.reshape(next_state, [1, 81])\n",
        "\n",
        "      # Synchronize Agent's board\n",
        "      env_agent.board.update_cell(random_action, 2)  # Random is Player 2 in Agent's board\n",
        "\n",
        "      # Print the boards\n",
        "      # print(\"\\nAgent's Board:\")\n",
        "      # env_agent.render()\n",
        "      # print(\"\\nRandom Agent's Board:\")\n",
        "      # env_random.render()\n",
        "\n",
        "      if done:\n",
        "          if reward == 1:\n",
        "              ctr_random+=1\n",
        "              # print(\"Random Agent wins!\")\n",
        "          else:\n",
        "              ctr_draw+=1\n",
        "              # print(\"It's a tie!\")\n",
        "          break\n",
        "\n",
        "      # Update the last move and Agent's state\n",
        "      last_move = random_action\n",
        "      state_agent = env_agent.board.board.flatten()\n",
        "      state_agent = np.reshape(state_agent, [1, 81])\n",
        "\n",
        "  # print(\"\\nGame Over.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-NUd5mnT5AMm"
      },
      "execution_count": 89,
      "outputs": [],
      "id": "-NUd5mnT5AMm"
    },
    {
      "cell_type": "code",
      "source": [
        "print(ctr_agent, ctr_draw, ctr_random)"
      ],
      "metadata": {
        "outputId": "d23841bd-3858-49e7-f53a-b7a03654bf55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEfRiGMn5Wnq"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 100 0\n"
          ]
        }
      ],
      "id": "vEfRiGMn5Wnq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM VS RANDOM"
      ],
      "metadata": {
        "id": "fmBtpNZs5JG1"
      },
      "id": "fmBtpNZs5JG1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent 1 With Complete Model Exploitation"
      ],
      "metadata": {
        "id": "J0KEiMM85JG3"
      },
      "id": "J0KEiMM85JG3"
    },
    {
      "cell_type": "code",
      "source": [
        "env_agent = UltimateTicTacToeEnv()\n",
        "model_path = \"dqn_agent_weights.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n",
        "agent = DQNAgent(state_size=82, action_size=81)\n",
        "agent.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca115683-a5e0-4f4e-d3ae-d8d204c47f68",
        "id": "IYJyuKME5JG3"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-a074415e080c>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "id": "IYJyuKME5JG3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent 2 With Complete Model Exploration"
      ],
      "metadata": {
        "id": "7BJwjgYm5JG3"
      },
      "id": "7BJwjgYm5JG3"
    },
    {
      "cell_type": "code",
      "source": [
        "env_random = UltimateTicTacToeEnv()\n",
        "model_path = \"dqn_agent_weights.pth\"\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n",
        "random_agent = DQNAgent(state_size=82, action_size=81)\n",
        "random_agent.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "outputId": "9cb53586-0bb1-409e-eeea-8853b25d0922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGpJ5AJR5JG3"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-92-85fefecb8ce1>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))  # Adjust for GPU if needed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "id": "GGpJ5AJR5JG3"
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, set epsilon to 0 so the agent uses its learned policy\n",
        "agent.epsilon = 1\n",
        "# After training, set epsilon to 1 so the random agent never uses its learned policy\n",
        "random_agent.epsilon = 1\n",
        "\n",
        "random.seed(16)\n",
        "np.random.seed(16)\n",
        "torch.manual_seed(16)\n",
        "\n",
        "#Initialize a counter\n",
        "ctr_agent = 0\n",
        "ctr_draw = 0\n",
        "ctr_random = 0\n",
        "\n",
        "\n",
        "for _ in range(100) :\n",
        "  # Initialize the environment\n",
        "  state_agent = env_agent.reset()\n",
        "  state_agent = np.reshape(state_agent, [1, 81])\n",
        "  state_random = env_random.reset()\n",
        "  state_random = np.reshape(state_random, [1, 81])\n",
        "  last_move = -1\n",
        "\n",
        "  done = False\n",
        "\n",
        "  # print(\"\\nStarting a new game of Ultimate Tic-Tac-Toe!\")\n",
        "  # print(\"Random is Player 2 (O), and the Agent is Player 1 (X).\")\n",
        "\n",
        "  while not done:\n",
        "      # Agent's turn\n",
        "      # print(\"\\nAgent's Turn:\")\n",
        "      agent_action = agent.act(state_agent, last_move)\n",
        "\n",
        "      # Apply Agent's action\n",
        "      next_state, reward, done, _ = env_agent.step(agent_action)\n",
        "      state_agent = np.reshape(next_state, [1, 81])\n",
        "\n",
        "      # Update the last move and synchronize Random's board\n",
        "      last_move = agent_action\n",
        "      env_random.board.update_cell(agent_action, 2)  # Agent is Player 1 in Random's board\n",
        "\n",
        "      # Update Random Agent's state\n",
        "      state_random = env_random.board.board.flatten()\n",
        "      state_random = np.reshape(state_random, [1, 81])\n",
        "\n",
        "      # Print the boards\n",
        "      # print(\"\\nAgent's Board:\")\n",
        "      # env_agent.render()\n",
        "      # print(\"\\nRandom Agent's Board:\")\n",
        "      # env_random.render()\n",
        "\n",
        "      if done:\n",
        "          if reward == 1:\n",
        "              ctr_agent+=1\n",
        "              # print(\"Agent wins!\")\n",
        "          else:\n",
        "              ctr_draw+=1\n",
        "              # print(\"It's a tie!\")\n",
        "          break\n",
        "\n",
        "      # Random Agent's turn\n",
        "      # print(\"\\nRandom Agent's Turn:\")\n",
        "      random_action = random_agent.act(state_random, last_move)\n",
        "\n",
        "      # Apply Random Agent's action\n",
        "      next_state, reward, done, _ = env_random.step(random_action)\n",
        "      state_random = np.reshape(next_state, [1, 81])\n",
        "\n",
        "      # Synchronize Agent's board\n",
        "      env_agent.board.update_cell(random_action, 2)  # Random is Player 2 in Agent's board\n",
        "\n",
        "      # Print the boards\n",
        "      # print(\"\\nAgent's Board:\")\n",
        "      # env_agent.render()\n",
        "      # print(\"\\nRandom Agent's Board:\")\n",
        "      # env_random.render()\n",
        "\n",
        "      if done:\n",
        "          if reward == 1:\n",
        "              ctr_random+=1\n",
        "              # print(\"Random Agent wins!\")\n",
        "          else:\n",
        "              ctr_draw+=1\n",
        "              # print(\"It's a tie!\")\n",
        "          break\n",
        "\n",
        "      # Update the last move and Agent's state\n",
        "      last_move = random_action\n",
        "      state_agent = env_agent.board.board.flatten()\n",
        "      state_agent = np.reshape(state_agent, [1, 81])\n",
        "\n",
        "  # print(\"\\nGame Over.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dmd_il_j5JG3"
      },
      "execution_count": 93,
      "outputs": [],
      "id": "dmd_il_j5JG3"
    },
    {
      "cell_type": "code",
      "source": [
        "print(ctr_agent, ctr_draw, ctr_random)"
      ],
      "metadata": {
        "outputId": "5259c462-129a-4b9d-b3df-710c0c7b92de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTB8Dcar5znx"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 91 4\n"
          ]
        }
      ],
      "id": "DTB8Dcar5znx"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}